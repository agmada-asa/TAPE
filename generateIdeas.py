"""
generateIdeas.py

Module responsible for chained semantic content analysis. Takes parsed transcripts 
and interacts with the local Llama 3.2 model via the Ollama python client 
to generate timestamped social media clip ideas.
"""

import os
from ollama import chat
from ollama import ChatResponse


def generate_social_media_post_ideas(transcript_chunk: str) -> str:
    """
    Sends a chunk of a transcript to the local Llama 3.2 model to generate social 
    media clip ideas. Prompts the model to act as a content creator and extract 
    viral, educational, or highly engaging moments.

    Args:
        transcript_chunk (str): A chunk of text from the source transcript.

    Returns:
        str: The generated markdown text containing post ideas and timestamps.
    """
    response: ChatResponse = chat(model='llama3.2', messages=[
        {
            'role': 'system',
            'content': (
                'You are a content creator who helps creators transform podcast transcripts '
                'into engaging social media content. Your role is to generate compelling Instagram '
                'reels and post ideas that maximize reach, grow the audience, and drive engagement. '
                'Focus on creating shareable, educational content with clear timestamps.'
            )
        },
        {
            'role': 'user',
            'content': (
                f"Please generate Instagram reel and post ideas from this transcript with example "
                f"content from the transcript itself. The content should be engaging, educational, "
                f"and shareable. Please include the timestamps of where the content is from:\n\n"
                f"{transcript_chunk}"
            ),
        },
    ])

    return response.message.content


def read_transcript_from_file(file_path: str) -> str:
    """
    Reads and extracts text from a supported transcript file format (.srt or .docx).

    Args:
        file_path (str): The absolute or relative path to the transcript file.

    Returns:
        str: The extracted raw text of the transcript.
        
    Raises:
        ValueError: If the file format is not .srt or .docx.
        FileNotFoundError: If the file does not exist.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Transcript file not found: {file_path}")

    if file_path.lower().endswith('.srt'):
        # For our semantic search, a raw read of the SRT is acceptable as the LLM 
        # naturally parses the timestamps. For strict text extraction, a dedicated 
        # SRT parser would be used.
        with open(file_path, 'r', encoding='utf-8') as f:
            transcript = f.read()
    elif file_path.lower().endswith('.docx'):
        # Note: python-docx must be installed for this to work
        try:
            import docx
        except ImportError:
            raise ImportError("The 'python-docx' library is required to read .docx files.")
            
        doc = docx.Document(file_path)
        transcript = "\n".join([para.text for para in doc.paragraphs])
    else:
        raise ValueError("Unsupported file format. Please use .srt or .docx files.")

    return transcript


def split_transcript(transcript: str) -> list:
    """
    Splits a large transcript document into smaller, manageable chunks suitable 
    for an LLM context window. Currently relies on double newlines to split paragraphs.

    Args:
        transcript (str): The full raw transcript string.

    Returns:
        list: A list of string chunks, each roughly up to 500 characters in length.
    """
    # Split by double newlines (typical paragraph or SRT block separation)
    parts = transcript.split("\n\n")
    sections = []
    
    current_section = ""

    for part in parts:
        # Accumulate parts until the chunk size exceeds 500 characters
        if len(current_section) < 500:
            if current_section:
                current_section += "\n\n" + part
            else:
                current_section = part
        else:
            sections.append(current_section)
            current_section = part

    # Append any remaining text as the final chunk
    if current_section:
        sections.append(current_section)

    return sections


def generate_content_ideas(path_to_transcript: str) -> list:
    """
    The main driver function for semantic content analysis. It reads a transcript file,
    splits it into context-window appropriate chunks, and batches those chunks to the
    LLM to generate a series of viral clip ideas.

    Args:
        path_to_transcript (str): Path to the .srt or .docx file generated by Whisper (or manual).

    Returns:
        list: A list containing LLM-generated recommendations for each transcript chunk.
    """
    # 1. Read transcript from the file
    transcript = read_transcript_from_file(path_to_transcript)

    # 2. Chunk transcript for Llama 3.2 processing
    splits = split_transcript(transcript)

    ideas = []

    # 3. Analyze each chunk individually
    # Note: Depending on transcript size, this may take a significant amount of time 
    # and might warrant concurrent requests if the Ollama instance supports concurrency.
    for split in splits:
        post_ideas = generate_social_media_post_ideas(split)
        ideas.append(post_ideas)

    return ideas